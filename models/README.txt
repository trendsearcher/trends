названия скриптов отражают их содержание относительно вот этой истории изменений:



К нашей задаче было 2 подхода: использовать картинки в '1d' CNN и цифры, характеризующие картинки. Есть также 2 варианта выставления оценок. 
Пока применяем топорную классификацию up/down для простоты. Классы почти идеально 50/50.. Когда уткнемся в потолок, тогда введем 3й класс и 
попробуем выехать на той же архитектуре. 

На практике оказалось, что цифры работают в лог регрессии, лесах и dnn на игрушечных сетах в 1000 примеров (average acc на кроссвалидации 54%) 
на 2 разных наборах данных.

СNN на картинках (average acc на кроссвалидации 54%) на 2 разных наборах данных. 



Проверим, не идиоты ли мы?



Возможно, есть баг? сгенерируем белый шум и сделаем броуновское движение на псевдослучайных цифрах. И прогоним эти данные через все этапы.
Нет. Мы молодцы. проверка дает ровно 50%.
Теперь проверим, не делаем ли мы лишнюю предподготовку? Может можно обойтись рандомными картинками и так все будет работать. Для проверки 
целесообразности именно такого нашего подхода к задаче попробовал убрать хвост паттерна длиной в 1/2 и во втором случае нарастил паттерн на 1/2. 
В обоих случаях метрики падают в 2-3 раза где-то. Поварьировал разрешение картинок на вход в СNN [500, 1000, 1500] сегментов в длину - 1000 - 
лучше так и оставим.
Интересно, что точность моделек почти идеально совпадала на игрушечных датасатех Логично было проверить, совпадают ли угадываемые моделью
примеры? Совпадают они лишь на 80+-%
Поэтому решил объединить входы моделей в 1 сетку и проверить, есть ли эффект синергии? - не вышло. те же (average acc на кроссвалидации 54%)
Ладно. тогда попробуем дать равные голоса для dnn и cnn и сделать ансамбль. Получили 55% на 3 разных наборах данных.
В дальнейшем будем использовать этот ансамбль, пока ничего лучше не придумано.
 

Интересно, что на 15000 (4года истории) DNN на цифрах почти не работает (если паттерны имеют стационарные характеристики, то странно, надо 
проверить это при последовательном кормлении модели слева направо).
А вот CNN на картинках дает все те же 54%.
Ансамбль - 54%

Есть стойкое ощущение, что мусора у нас много. Ввел 3-й класс для него (метод разметки обговаривали много раз - писать тут не буду) Вроде работает, 
картинки можно зачекать в ноутбуке. АУКи классные по 0.8 на каждый класс. Вроде надо радоваться, но я уже параноик и поэтому проверил результат на 
все том же броуновском движении. И получился какой-то бред. на белом шуме оно там чему-то обучается и даже дает неплохие ауки на Сбере, 
обучившись на шуме. Не могу найти баг, либо логическую ошибку. Меня этот выводит из себя.


===============================================================================================================================================
                                                   попытка отделить предсказываемые примеры от шумовых
===============================================================================================================================================
Это параллельная ветка развития. Придумал 2 подхода. 

Первый подход к проблеме.

Делаем 2 набора данных: A и B, которые никак друг с другом не связаны и не имеют ничего общего. Для каждого из них проводим следующие манипуляции: много раз случайным образом отбираем трейн и тест из каждого набора данных. На каждой генерации трейна и теста запускаем обучение на CNN и,
пропустив 15-20% эпох начинаем на каждой эпохе сравнивать предсказанные примеры из (predicted test) с (true test). Если на этих 85-80% эпох какие-то оценки примеров стабильно совпадают с оригинальными, то эти примеры не являются шумом. Прогнав на множестве случайно отобранных таким вот образом тестовых множеств все вычисления, получим оценку того, какие примеры CNN стабильно угадывает а какие  50/50. на основе этого сделаем оценки 'мусорности примеров'. Проверим для начала, как распределены оценки. Если по Гауссу, то мы сделали ерунду. QQ плот ненормальный, есть тяжелые хвосты. Бинаризуем оценки мусорности и проверим, предсказывает ли найронка, обученная на A, примеры из B.

Резюме: нет.

Второй подход к проблеме:
  
Много раз случайным образом отбираем трейн и тест из некого набора данных. Если для какой-то выборки получаем метрики выше пороговых (что означает, что в данном разбиении хорошие нешумовые примеры распределились удачно между трейном и тестом), то начинаем последовательно изменять оценки пример  за примером из тестового множества. Испортив оценки на хороших примерах, мы испортим метрики, испортив их на шуме, не затроним метрики. Так можно будет дифференцировать хорошие примеры от шумовых.


