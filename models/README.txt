названия скриптов отражают их содержание относительно вот этой истории изменений:



К нашей задаче было 2 подхода: использовать картинки в '1d' CNN и цифры, характеризующие картинки. Есть также 2 варианта выставления оценок. 
Пока применяем топорную классификацию up/down для простоты. Классы почти идеально 50/50.

На практике оказалось, что цифры работают в лог регрессии, лесах и dnn на игрушечных сетах в 1000 примеров (average acc на кроссвалидации 54%) 
на 2 разных наборах данных.

СNN на картинках (average acc на кроссвалидации 54%) на 2 разных наборах данных. 



Проверим, не идиоты ли мы?



Возможно, есть баг? сгенерируем белый шум и сделаем броуновское движение на псевдослучайных цифрах. И прогоним эти данные через все этапы.
Нет. Мы молодцы. проверка дает ровно 50%.
Интересно, что точность моделек почти идеально совпадала на игрушечных датасатех Логично было проверить, совпадают ли угадываемые моделью
примеры? Совпадают они лишь на 80+-%
Поэтому решил объединить входы моделей в 1 сетку и проверить, есть ли эффект синергии? - не вышло. те же (average acc на кроссвалидации 54%)
Ладно. тогда попробуем дать равные голоса для dnn и cnn и сделать ансамбль. Получили 55% на 3 разных наборах данных.
В дальнейшем будем использовать этот ансамбль, пока ничего лучше не придумано.
 

Интересно, что на 15000 (4года истории) DNN на цифрах почти не работает (если паттерны имеют стационарные характеристики, то странно, надо 
проверить это при последовательном кормлении модели слева направо).
А вот CNN на картинках дает все те же 54%.
Ансамбль - 54%




